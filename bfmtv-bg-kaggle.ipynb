{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"337b45f2edf748a6aca08192bc1f838f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deace488020040bea117137cd181cdd0","IPY_MODEL_70da932240e14c4fb51248cb781624ca","IPY_MODEL_f7398e4da7944073a9edb8804b1dd970"],"layout":"IPY_MODEL_4323a28d68ea4064b4abd8eea2b5991d"}},"deace488020040bea117137cd181cdd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5e8708eeea54e52b6307cc71bb375b3","placeholder":"​","style":"IPY_MODEL_2b2d6ec5fabd4df6a981474fec767e36","value":"Map: 100%"}},"70da932240e14c4fb51248cb781624ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce40996bfd0944ac9a4ef00657f41880","max":289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e785f31e65c84a75863625148d78ce66","value":289}},"f7398e4da7944073a9edb8804b1dd970":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20e6e6c01eb34e3eb306ad8a4af3f4b2","placeholder":"​","style":"IPY_MODEL_bdb00611aae24b82afbf162938f44be7","value":" 289/289 [00:00&lt;00:00, 556.84 examples/s]"}},"4323a28d68ea4064b4abd8eea2b5991d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5e8708eeea54e52b6307cc71bb375b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b2d6ec5fabd4df6a981474fec767e36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce40996bfd0944ac9a4ef00657f41880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e785f31e65c84a75863625148d78ce66":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20e6e6c01eb34e3eb306ad8a4af3f4b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdb00611aae24b82afbf162938f44be7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff24161ad38b443f9d3fec5f2939f586":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95555b2edf2c47139908922dfa5e89d0","IPY_MODEL_921be156b6484289924467dcd63622f1","IPY_MODEL_3de076b68a79468a9d8614067aeb1ce5"],"layout":"IPY_MODEL_f3b9e5f74c6e45649c221592b239d9c8"}},"95555b2edf2c47139908922dfa5e89d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e61bde94d104a678e6c6c59762c03e8","placeholder":"​","style":"IPY_MODEL_0e40d1b82e934ca9823729d3e5977f67","value":"Map: 100%"}},"921be156b6484289924467dcd63622f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a05a3ce88f754883afae59695f0d8b23","max":73,"min":0,"orientation":"horizontal","style":"IPY_MODEL_519824295f494b17ada5d163d191f006","value":73}},"3de076b68a79468a9d8614067aeb1ce5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c63e2cee41f4fddaa7ef9dcb1c3cb6a","placeholder":"​","style":"IPY_MODEL_844fd2eb62334e4087c8687561761361","value":" 73/73 [00:00&lt;00:00, 349.57 examples/s]"}},"f3b9e5f74c6e45649c221592b239d9c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e61bde94d104a678e6c6c59762c03e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e40d1b82e934ca9823729d3e5977f67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a05a3ce88f754883afae59695f0d8b23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"519824295f494b17ada5d163d191f006":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c63e2cee41f4fddaa7ef9dcb1c3cb6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"844fd2eb62334e4087c8687561761361":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U datasets accelerate peft transformers trl bitsandbytes","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DD8C1HnguBvd","outputId":"dfc0ad57-ea86-4e31-ddbc-ebd92769a790","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:33:38.288157Z","iopub.execute_input":"2025-02-10T12:33:38.288522Z","iopub.status.idle":"2025-02-10T12:33:42.228613Z","shell.execute_reply.started":"2025-02-10T12:33:38.288486Z","shell.execute_reply":"2025-02-10T12:33:42.227643Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# inspiré de https://github.com/blancsw/deep_4_all/blob/main/cours/TP/text/sft_train_gpu.ipynb\n\nfrom accelerate import PartialState\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n\ncheckpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n\n# messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\n# input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n# print(input_text)\n# inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n# outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n# print(tokenizer.decode(outputs[0]))","metadata":{"id":"p-vDXNG3slrY","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:33:45.278241Z","iopub.execute_input":"2025-02-10T12:33:45.278538Z","iopub.status.idle":"2025-02-10T12:34:16.315975Z","shell.execute_reply.started":"2025-02-10T12:33:45.278513Z","shell.execute_reply":"2025-02-10T12:34:16.315024Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2956fdc2e9d42aaacd7b11fd3a8b695"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a505357f9e1476ab897538f8942fd4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c83a78e2ff944ed8f2325f832594ac5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e4ea5e536af4999829b1c1b11929e59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b38457b548476ebec6370002484136"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77d2059737304107a1dcc19d56e6b42d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0122dc60bcc6449c83b7a5132fcfc1ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18acd72622854dedba550a821f906ee6"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Charger le dataset\n\nds = load_dataset(\"IJUN/FakeNews\")\n\nsplit_ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n\n# Access train and test splits\ntrain_dataset = split_ds[\"train\"]\ntest_dataset = split_ds[\"test\"]\n","metadata":{"id":"NuxTCuO_tDUJ","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:36:19.104942Z","iopub.execute_input":"2025-02-10T12:36:19.105291Z","iopub.status.idle":"2025-02-10T12:36:20.297739Z","shell.execute_reply.started":"2025-02-10T12:36:19.105270Z","shell.execute_reply":"2025-02-10T12:36:20.296865Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"dataset.jsonl:   0%|          | 0.00/1.79M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7653b8aa6f15400d91dbdfe08b87b8e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/362 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da41c9df40984e32bea8496648b56439"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"response_template = \"<|im_start|>assistant\\n\"\ninstruction_template = \"<|im_start|>user\\n\"\nPROMPT_TEMPLATE = \"\"\"Query: {query}\n\n\"\"\"\n\ndef formatting_prompts_func(example):\n    \"\"\"\n    Converts each example into a conversation string using the tokenizer's chat template.\n    Assumes each example contains lists under \"instruction\" and \"output\".\n    \"\"\"\n    output_texts = []\n    for i in range(len(example[\"input\"])):\n        # Build a conversation with a user message and an assistant reply.\n        messages = [\n            {\n                \"role\":    \"system\",\n                \"content\": \"You are an expert journalist / fact-checker from Fox News and your goal is to fact-check the user query.\"\n                },\n            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(query=example[\"input\"][i])},\n            # Note: It is important that the assistant message content here does not\n            # include the assistant marker, because the chat template will insert it.\n            {\"role\": \"assistant\", \"content\": example[\"output\"][i]}\n            ]\n        # Use the chat template to generate the formatted text.\n        text = tokenizer.apply_chat_template(messages, tokenize=False)\n        output_texts.append(text)\n    return output_texts\n\ncollator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n                                           instruction_template=instruction_template,\n                                           tokenizer=tokenizer,\n                                           mlm=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZVlFknS1_Z9","outputId":"fb0b735a-68de-428f-fe0b-8277a0684df2","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:36:24.172050Z","iopub.execute_input":"2025-02-10T12:36:24.172350Z","iopub.status.idle":"2025-02-10T12:36:24.185849Z","shell.execute_reply.started":"2025-02-10T12:36:24.172328Z","shell.execute_reply":"2025-02-10T12:36:24.184856Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:112: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"tokenizer.apply_chat_template([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    {\"role\": \"assistant\", \"content\": \"I am good, thank you.\"}\n    ], tokenize=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"z0YfBKnl2xBv","outputId":"d152d1bb-230b-4228-e723-5c9b49c88caa","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:36:32.600485Z","iopub.execute_input":"2025-02-10T12:36:32.600793Z","iopub.status.idle":"2025-02-10T12:36:32.611705Z","shell.execute_reply.started":"2025-02-10T12:36:32.600771Z","shell.execute_reply":"2025-02-10T12:36:32.610903Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI am good, thank you.<|im_end|>\\n'"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"### Lora Config","metadata":{"id":"C8gHMOQC3dqH"}},{"cell_type":"code","source":"from peft import LoraConfig\n\nlora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        target_modules=['o_proj', 'k_proj', 'q_proj', \"v_proj\"],\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n      )","metadata":{"id":"KcY5cHYR3a7Q","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:36:54.970757Z","iopub.execute_input":"2025-02-10T12:36:54.971106Z","iopub.status.idle":"2025-02-10T12:36:54.975082Z","shell.execute_reply.started":"2025-02-10T12:36:54.971081Z","shell.execute_reply":"2025-02-10T12:36:54.974279Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Wandb (optionnel)\nPour visualiser certaines données sur l'entraînement","metadata":{"id":"M_-5A4Se3yEF"}},{"cell_type":"code","source":"# import wandb\n\n# wandb.login()","metadata":{"id":"goxaeX6Y31up","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SFT Trainer config","metadata":{"id":"Lb-1DEl23-kS"}},{"cell_type":"code","source":"OUTPUT_DIR = checkpoint.split(\"/\")[-1] + \"-structure-output\"\n\n# setup the trainer\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        args=SFTConfig(\n                per_device_train_batch_size=2,\n                gradient_accumulation_steps=4,\n                warmup_steps=100,\n                max_steps=1500,\n                learning_rate=0.0001,\n                lr_scheduler_type=\"cosine\",\n                eval_strategy=\"steps\",\n                eval_steps=150,\n                weight_decay=0.01,\n                bf16=True,\n                logging_strategy=\"steps\",\n                logging_steps=10,\n                output_dir=\"./\" + OUTPUT_DIR,\n                optim=\"paged_adamw_8bit\",\n                seed=42,\n                run_name=f\"train-{OUTPUT_DIR}\",\n                report_to=\"none\",\n                save_steps=31,\n                save_total_limit=4,\n                ),\n        peft_config=lora_config,\n        formatting_func=formatting_prompts_func,\n        data_collator=collator,\n        )","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["337b45f2edf748a6aca08192bc1f838f","deace488020040bea117137cd181cdd0","70da932240e14c4fb51248cb781624ca","f7398e4da7944073a9edb8804b1dd970","4323a28d68ea4064b4abd8eea2b5991d","f5e8708eeea54e52b6307cc71bb375b3","2b2d6ec5fabd4df6a981474fec767e36","ce40996bfd0944ac9a4ef00657f41880","e785f31e65c84a75863625148d78ce66","20e6e6c01eb34e3eb306ad8a4af3f4b2","bdb00611aae24b82afbf162938f44be7","ff24161ad38b443f9d3fec5f2939f586","95555b2edf2c47139908922dfa5e89d0","921be156b6484289924467dcd63622f1","3de076b68a79468a9d8614067aeb1ce5","f3b9e5f74c6e45649c221592b239d9c8","7e61bde94d104a678e6c6c59762c03e8","0e40d1b82e934ca9823729d3e5977f67","a05a3ce88f754883afae59695f0d8b23","519824295f494b17ada5d163d191f006","8c63e2cee41f4fddaa7ef9dcb1c3cb6a","844fd2eb62334e4087c8687561761361"]},"id":"f4hdveS04CCS","outputId":"0e0325dc-1659-4bef-a1b3-5e705629c4bb","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:36:58.419776Z","iopub.execute_input":"2025-02-10T12:36:58.420144Z","iopub.status.idle":"2025-02-10T12:36:59.471444Z","shell.execute_reply.started":"2025-02-10T12:36:58.420117Z","shell.execute_reply":"2025-02-10T12:36:59.470827Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/289 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c0ca445ed049a2b548892a5f56d05e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/73 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bb24dca9c5e4caf9e037d790ba3fad7"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"import os\nfrom transformers import is_torch_xpu_available, is_torch_npu_available\nimport torch\n\n# Lancement du processus d'entraînement du modèle.\n# Ici, 'trainer.train()' déclenche la phase de fine-tuning,\n# dans laquelle les paramètres du modèle sont ajustés sur une tâche spécifique\n# en utilisant des données d'entraînement pertinentes.\ntrainer.train()\n\n# Une fois l'entraînement terminé, on sauvegarde l'adaptateur LoRA (fine-tuning léger).\n# LoRA (Low-Rank Adaptation) est une technique destinée à fine-tuner les grands\n# modèles en modifiant uniquement un sous-ensemble restreint de paramètres.\nfinal_checkpoint_dir = os.path.join(OUTPUT_DIR, \"final_checkpoint\")\ntrainer.model.save_pretrained(final_checkpoint_dir)\n\n# Nettoyage des ressources mémoire pour libérer l'espace GPU ou autres accélérateurs,\n# ce qui est utile avant de fusionner l'adaptateur LoRA avec le modèle de base.\ndel model  # Suppression explicite du modèle de la mémoire.\n\n# Vider les caches des accélérateurs (XPU, NPU ou GPU en fonction de la disponibilité).\n# Cela optimise l'utilisation future des ressources.\nif is_torch_xpu_available():\n    torch.xpu.empty_cache()  # Vide les caches spécifiques pour XPU.\nelif is_torch_npu_available():\n    torch.npu.empty_cache()  # Vide les caches spécifiques pour NPU.\nelse:\n    torch.cuda.empty_cache()  # Vide les caches GPU standard.","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"RjawKwA64Y_U","outputId":"4b82ed3b-fac0-4f4c-9c1d-d8e90aa27318","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T12:37:05.060703Z","iopub.execute_input":"2025-02-10T12:37:05.061029Z","iopub.status.idle":"2025-02-10T13:30:58.602505Z","shell.execute_reply.started":"2025-02-10T12:37:05.061007Z","shell.execute_reply":"2025-02-10T13:30:58.601813Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1500/1500 53:50, Epoch 40/42]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>150</td>\n      <td>1.014600</td>\n      <td>1.083068</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.868400</td>\n      <td>0.971830</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.784100</td>\n      <td>0.943105</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.678900</td>\n      <td>0.943919</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.646200</td>\n      <td>0.962503</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.597200</td>\n      <td>0.979547</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.547400</td>\n      <td>1.000270</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.532600</td>\n      <td>1.015623</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.519200</td>\n      <td>1.020814</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.543500</td>\n      <td>1.021109</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Chargement du modèle adapté (en incluant l'adaptateur LoRA) pour effectuer une fusion\n# avec le modèle de base. Cela permet de sauvegarder un modèle autonome optimisé.\nfrom peft import AutoPeftModelForCausalLM\n\n# Chargement du modèle préalablement sauvegardé depuis le répertoire OUTPUT_DIR.\n# Les paramètres 'device_map' et 'torch_dtype' permettent d'optimiser le chargement :\n# - 'device_map=\"auto\"' ajuste automatiquement le placement sur le GPU, CPU ou autre.\n# - 'torch_dtype=torch.bfloat16' utilise un format numérique bfloat16, qui réduit\n#    la mémoire nécessaire tout en maintenant des performances stables.\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n        final_checkpoint_dir,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16\n        )\n\n# Fusion de l'adaptateur LoRA directement dans le modèle de base,\n# afin de produire un modèle final unique tout en réduisant ses redondances.\nmodel = model.merge_and_unload()\n\n# Sauvegarde du modèle fusionné dans un répertoire spécifique.\n# 'safe_serialization=True' garantit que le modèle est stocké au format sûr,\n# pour une compatibilité future et une intégrité des données.\noutput_merged_dir = os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\")\nmodel.save_pretrained(output_merged_dir, safe_serialization=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T13:31:04.776404Z","iopub.execute_input":"2025-02-10T13:31:04.776704Z","iopub.status.idle":"2025-02-10T13:31:07.884018Z","shell.execute_reply.started":"2025-02-10T13:31:04.776682Z","shell.execute_reply":"2025-02-10T13:31:07.883303Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/working/SmolLM2-360M-Instruct-structure-output/final_merged_checkpoint\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T14:10:21.826270Z","iopub.execute_input":"2025-02-10T14:10:21.826571Z","iopub.status.idle":"2025-02-10T14:10:23.080135Z","shell.execute_reply.started":"2025-02-10T14:10:21.826547Z","shell.execute_reply":"2025-02-10T14:10:23.079245Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"Gor-bepis/fact-checker-bfmtg-v1\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T14:22:41.001934Z","iopub.execute_input":"2025-02-10T14:22:41.002274Z","iopub.status.idle":"2025-02-10T14:23:17.844551Z","shell.execute_reply.started":"2025-02-10T14:22:41.002250Z","shell.execute_reply":"2025-02-10T14:23:17.843689Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4209fd289ddf4ed9a80b78da11acda1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d28ca867b204a7ca08bef6eaa23272e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6205b780add5407c9dab6e98d5d1b7ea"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T14:14:17.918652Z","iopub.execute_input":"2025-02-10T14:14:17.919120Z","iopub.status.idle":"2025-02-10T14:14:17.944713Z","shell.execute_reply.started":"2025-02-10T14:14:17.919078Z","shell.execute_reply":"2025-02-10T14:14:17.943609Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d13c67bc04034374921c8a760a6b532e"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"model.push_to_hub(\"Gor-bepis/fact-checker-bfmtg-v1\")\ntokenizer.push_to_hub(\"Gor-bepis/fact-checker-bfmtg-v1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T14:15:51.828065Z","iopub.execute_input":"2025-02-10T14:15:51.828420Z","iopub.status.idle":"2025-02-10T14:16:44.170371Z","shell.execute_reply.started":"2025-02-10T14:15:51.828390Z","shell.execute_reply":"2025-02-10T14:16:44.169625Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fd1d786fe6242cd98f31465b3260cd1"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Gor-bepis/fact-checker-bfmtg-v1/commit/44732520cb2b89cb4e1ccc042fc6b4ff18fba138', commit_message='Upload tokenizer', commit_description='', oid='44732520cb2b89cb4e1ccc042fc6b4ff18fba138', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Gor-bepis/fact-checker-bfmtg-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='Gor-bepis/fact-checker-bfmtg-v1'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"messages = [\n            {\n                \"role\":    \"system\",\n                \"content\": \"You are an expert journalist / fact-checker from Fox News and your goal is to fact-check the user query.\"\n                },\n    {\"role\": \"user\", \"content\": \"Biden: Trump should apologize for Obama wiretap claim Joe Biden said President Donald Trump should apologize to former President Barack Obama for his accusation that the Obama administration wiretapped Trump Tower. \\\"Any gentleman would,\\\" the former vice president said. Obama, via a spokesperson, denied that he wiretapped Trump Tower after the current president tweeted the accusation. FBI Director James Comey this week also testified that there's no evidence the Obama administration surveilled Trump during the leadup to the e2016 election\"}\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T14:10:46.710148Z","iopub.execute_input":"2025-02-10T14:10:46.710485Z","iopub.status.idle":"2025-02-10T14:10:46.714121Z","shell.execute_reply.started":"2025-02-10T14:10:46.710457Z","shell.execute_reply":"2025-02-10T14:10:46.713286Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"input_text = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(input_text)\nprint(\"----------------- Generated text -----------------\")\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=1024, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T14:23:51.124792Z","iopub.execute_input":"2025-02-10T14:23:51.125157Z","iopub.status.idle":"2025-02-10T14:24:23.589192Z","shell.execute_reply.started":"2025-02-10T14:23:51.125132Z","shell.execute_reply":"2025-02-10T14:24:23.588378Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are an expert journalist / fact-checker from Fox News and your goal is to fact-check the user query.<|im_end|>\n<|im_start|>user\nBiden: Trump should apologize for Obama wiretap claim Joe Biden said President Donald Trump should apologize to former President Barack Obama for his accusation that the Obama administration wiretapped Trump Tower. \"Any gentleman would,\" the former vice president said. Obama, via a spokesperson, denied that he wiretapped Trump Tower after the current president tweeted the accusation. FBI Director James Comey this week also testified that there's no evidence the Obama administration surveilled Trump during the leadup to the e2016 election<|im_end|>\n\n----------------- Generated text -----------------\n<|im_start|>system\nYou are an expert journalist / fact-checker from Fox News and your goal is to fact-check the user query.<|im_end|>\n<|im_start|>user\nBiden: Trump should apologize for Obama wiretap claim Joe Biden said President Donald Trump should apologize to former President Barack Obama for his accusation that the Obama administration wiretapped Trump Tower. \"Any gentleman would,\" the former vice president said. Obama, via a spokesperson, denied that he wiretapped Trump Tower after the current president tweeted the accusation. FBI Director James Comey this week also testified that there's no evidence the Obama administration surveilled Trump during the leadup to the e2016 election<|im_end|>\n<|im_start|>assistant\n<end>fake</end>\n\n**Reasons:**  \n1. **Source Verification**: The claim lacks credible sourcing from reputable news outlets or official statements from Biden or the Biden-Harris administration.  \n2. **Cross-Checking Facts**: There is no evidence or record of Biden or FBI Director James Comey making such a claim.  \n3. **Language and Tone**: The language is sensational and lacks neutrality, which is often a red flag for fake news.  \n4. **Misattribution**: The claim is attributed to \"Joe Biden,\" which is not a credible source.  \n5. **Timing and Context**: The claim does not align with Biden's public statements or actions during the Trump e2016 campaign.  \n\nBased on these steps, the content is identified as fake news.  \n\n[End]  \n\n[<end>fake</end>>]  \n\n**Reasons:**  \n * Inconsistencies with credible sources and historical facts.  \n * Unrealistic and exaggerated claims are easily detectable.  \n * Emotional manipulation is a common trait of fake news.  \n * Reputable news outlets have not reported this claim.  \n\n**End**  \n\n<end>fake</end>  \n\n**Reasons**  \n * No credible source supports the claim.  \n * The language is sensational and lacks neutrality.  \n * The context does not align with known events or Biden's actions during the Trump e2016 campaign.  \n * A detailed investigation would be required to verify the claim.  \n\n**End**  \n\n<end>fake</end>  \n\n**Reasons**  \n * Credible sources are not found.  \n * The claims are inconsistent with historical facts and credible reporting.  \n * The tone is exaggerated and lacks neutrality.  \n * The context does not align with known events or Biden's actions during the Trump e2016 campaign.  \n * A detailed investigation is required to verify the claim.  \n\n**End**  \n\n<end>fake</end>  \n\n**Reasons**  \n * Credible sources are not found.  \n * The claims are inconsistent with historical facts and credible reporting.  \n * The tone is exaggerated and lacks neutrality.  \n * The context does not align with known events or Biden's actions during the Trump e2016 campaign.  \n * A detailed investigation is required to verify the claim.  \n\n**End**  \n\n<end>fake</end>  \n\n**Reasons**  \n * Credible sources are not found.  \n * The claims are inconsistent with historical facts and credible reporting.  \n * The tone is exaggerated and lacks neutrality.  \n * The context does not align with known events or Biden's actions during the Trump e2016 campaign.  \n * A detailed investigation is required to verify the claim.  \n\n**End**  \n\n<end>fake</end>  \n\n**Reasons**  \n * Credible sources are not found.  \n * The claims are inconsistent with historical facts and credible reporting.  \n * The tone is exaggerated and lacks neutrality.  \n * The context does not align with known events or Biden's actions during the Trump e2016 campaign.  \n * A detailed investigation is required to verify the claim.  \n\n**End**  \n\n<end>fake</end>  \n\n**Reasons**  \n * Credible sources are not found.  \n * The claims are inconsistent with historical facts and credible reporting.  \n * The tone is exaggerated and lacks neutrality.  \n * The context does not align with known events or Biden's actions during the Trump e2016 campaign.  \n * A detailed investigation is required to verify the claim.  \n\n**End**  \n\n**End*fake**end***  \n\n**(CNN)  \n\n**(CNN.org)**  \n\n**(CNN.com)**  \n\n**(CNN.politics)**  \n\n**(CNN.politics.com)**  \n\n**(CNN.politics.e2016)**  \n\n**(CNN.e2016.com)**  \n\n**(CNN.e2016.politics)**  \n\n**(CNN.e2016.politics.com)**  \n\n**(CNN.e2016.politics.e2016)**  \n\n**(CNN.e2016.com)**  \n\n**(CNN.e2016.politics)**  \n\n**(CNN.e2016.politics.com)**  \n\n**(CNN.e2016.politics.e2016)**  \n\n**(CNN.e2016.politics.e2016)**  \n\n**(CNN.e2016.com)**  \n\n**(CNN.e2016.politics)**  \n\n**(CNN\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"### Push sur Hugging Face","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!huggingface-cli login\n!huggingface-cli repo create fact-checker-bfmtg --type model","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}