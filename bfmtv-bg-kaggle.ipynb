{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"337b45f2edf748a6aca08192bc1f838f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deace488020040bea117137cd181cdd0","IPY_MODEL_70da932240e14c4fb51248cb781624ca","IPY_MODEL_f7398e4da7944073a9edb8804b1dd970"],"layout":"IPY_MODEL_4323a28d68ea4064b4abd8eea2b5991d"}},"deace488020040bea117137cd181cdd0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5e8708eeea54e52b6307cc71bb375b3","placeholder":"​","style":"IPY_MODEL_2b2d6ec5fabd4df6a981474fec767e36","value":"Map: 100%"}},"70da932240e14c4fb51248cb781624ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce40996bfd0944ac9a4ef00657f41880","max":289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e785f31e65c84a75863625148d78ce66","value":289}},"f7398e4da7944073a9edb8804b1dd970":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20e6e6c01eb34e3eb306ad8a4af3f4b2","placeholder":"​","style":"IPY_MODEL_bdb00611aae24b82afbf162938f44be7","value":" 289/289 [00:00&lt;00:00, 556.84 examples/s]"}},"4323a28d68ea4064b4abd8eea2b5991d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5e8708eeea54e52b6307cc71bb375b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b2d6ec5fabd4df6a981474fec767e36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce40996bfd0944ac9a4ef00657f41880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e785f31e65c84a75863625148d78ce66":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20e6e6c01eb34e3eb306ad8a4af3f4b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdb00611aae24b82afbf162938f44be7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff24161ad38b443f9d3fec5f2939f586":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95555b2edf2c47139908922dfa5e89d0","IPY_MODEL_921be156b6484289924467dcd63622f1","IPY_MODEL_3de076b68a79468a9d8614067aeb1ce5"],"layout":"IPY_MODEL_f3b9e5f74c6e45649c221592b239d9c8"}},"95555b2edf2c47139908922dfa5e89d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e61bde94d104a678e6c6c59762c03e8","placeholder":"​","style":"IPY_MODEL_0e40d1b82e934ca9823729d3e5977f67","value":"Map: 100%"}},"921be156b6484289924467dcd63622f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a05a3ce88f754883afae59695f0d8b23","max":73,"min":0,"orientation":"horizontal","style":"IPY_MODEL_519824295f494b17ada5d163d191f006","value":73}},"3de076b68a79468a9d8614067aeb1ce5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c63e2cee41f4fddaa7ef9dcb1c3cb6a","placeholder":"​","style":"IPY_MODEL_844fd2eb62334e4087c8687561761361","value":" 73/73 [00:00&lt;00:00, 349.57 examples/s]"}},"f3b9e5f74c6e45649c221592b239d9c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e61bde94d104a678e6c6c59762c03e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e40d1b82e934ca9823729d3e5977f67":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a05a3ce88f754883afae59695f0d8b23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"519824295f494b17ada5d163d191f006":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c63e2cee41f4fddaa7ef9dcb1c3cb6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"844fd2eb62334e4087c8687561761361":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U datasets accelerate peft transformers trl bitsandbytes","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DD8C1HnguBvd","outputId":"dfc0ad57-ea86-4e31-ddbc-ebd92769a790","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:57:11.018163Z","iopub.execute_input":"2025-02-10T09:57:11.018617Z","iopub.status.idle":"2025-02-10T09:57:14.770368Z","shell.execute_reply.started":"2025-02-10T09:57:11.018573Z","shell.execute_reply":"2025-02-10T09:57:14.769224Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# inspiré de https://github.com/blancsw/deep_4_all/blob/main/cours/TP/text/sft_train_gpu.ipynb\n\nfrom accelerate import PartialState\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n\ncheckpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")\n\n# messages = [{\"role\": \"user\", \"content\": \"What is the capital of France.\"}]\n# input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n# print(input_text)\n# inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n# outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n# print(tokenizer.decode(outputs[0]))","metadata":{"id":"p-vDXNG3slrY","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:57:20.651482Z","iopub.execute_input":"2025-02-10T09:57:20.651878Z","iopub.status.idle":"2025-02-10T09:57:54.256653Z","shell.execute_reply.started":"2025-02-10T09:57:20.651845Z","shell.execute_reply":"2025-02-10T09:57:54.255975Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f31828b55454dd1b88031a069fef486"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ccfdd2b04e4d65a2789e3a8d16e6c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5665297730e457aad2a7e627a9376c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e34fc8e5c5614757a20c7cb77a8294a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7aee82f50a743908a09b3ce2972aaef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f4a7d1b027f4cf3bd3d981b5404ebd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bcbe05543004ae78b264a9905b7aa71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6587f82352402581af89af74a0b334"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Charger le dataset\n\nds = load_dataset(\"IJUN/FakeNews\")\n\nsplit_ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n\n# Access train and test splits\ntrain_dataset = split_ds[\"train\"]\ntest_dataset = split_ds[\"test\"]\n","metadata":{"id":"NuxTCuO_tDUJ","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:57:56.795540Z","iopub.execute_input":"2025-02-10T09:57:56.795840Z","iopub.status.idle":"2025-02-10T09:57:59.498299Z","shell.execute_reply.started":"2025-02-10T09:57:56.795817Z","shell.execute_reply":"2025-02-10T09:57:59.497592Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"dataset.jsonl:   0%|          | 0.00/1.79M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc68288b011a413f95169ce7df07f428"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/362 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b333844bd5a4bc08b0daf7b5e84d8e4"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"response_template = \"<|im_start|>assistant\\n\"\ninstruction_template = \"<|im_start|>user\\n\"\nPROMPT_TEMPLATE = \"\"\"Query: {query}\n\n\"\"\"\n\ndef formatting_prompts_func(example):\n    \"\"\"\n    Converts each example into a conversation string using the tokenizer's chat template.\n    Assumes each example contains lists under \"instruction\" and \"output\".\n    \"\"\"\n    output_texts = []\n    for i in range(len(example[\"input\"])):\n        # Build a conversation with a user message and an assistant reply.\n        messages = [\n            {\n                \"role\":    \"system\",\n                \"content\": \"You are an expert journalist / fact-checker from Fox News and your goal is to fact-check the user query.\"\n                },\n            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(query=example[\"input\"][i])},\n            # Note: It is important that the assistant message content here does not\n            # include the assistant marker, because the chat template will insert it.\n            {\"role\": \"assistant\", \"content\": example[\"output\"][i]}\n            ]\n        # Use the chat template to generate the formatted text.\n        text = tokenizer.apply_chat_template(messages, tokenize=False)\n        output_texts.append(text)\n    return output_texts\n\ncollator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n                                           instruction_template=instruction_template,\n                                           tokenizer=tokenizer,\n                                           mlm=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZVlFknS1_Z9","outputId":"fb0b735a-68de-428f-fe0b-8277a0684df2","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:58:20.892705Z","iopub.execute_input":"2025-02-10T09:58:20.893070Z","iopub.status.idle":"2025-02-10T09:58:20.899301Z","shell.execute_reply.started":"2025-02-10T09:58:20.893037Z","shell.execute_reply":"2025-02-10T09:58:20.898393Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"tokenizer.apply_chat_template([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    {\"role\": \"assistant\", \"content\": \"I am good, thank you.\"}\n    ], tokenize=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"z0YfBKnl2xBv","outputId":"d152d1bb-230b-4228-e723-5c9b49c88caa","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:58:24.340092Z","iopub.execute_input":"2025-02-10T09:58:24.340392Z","iopub.status.idle":"2025-02-10T09:58:24.351475Z","shell.execute_reply.started":"2025-02-10T09:58:24.340369Z","shell.execute_reply":"2025-02-10T09:58:24.350575Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI am good, thank you.<|im_end|>\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### Lora Config","metadata":{"id":"C8gHMOQC3dqH"}},{"cell_type":"code","source":"from peft import LoraConfig\n\nlora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        target_modules=['o_proj', 'k_proj', 'q_proj', \"v_proj\"],\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n      )","metadata":{"id":"KcY5cHYR3a7Q","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:58:26.650140Z","iopub.execute_input":"2025-02-10T09:58:26.650473Z","iopub.status.idle":"2025-02-10T09:58:26.654610Z","shell.execute_reply.started":"2025-02-10T09:58:26.650443Z","shell.execute_reply":"2025-02-10T09:58:26.653747Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Wandb (optionnel)\nPour visualiser certaines données sur l'entraînement","metadata":{"id":"M_-5A4Se3yEF"}},{"cell_type":"code","source":"# import wandb\n\n# wandb.login()","metadata":{"id":"goxaeX6Y31up","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SFT Trainer config","metadata":{"id":"Lb-1DEl23-kS"}},{"cell_type":"code","source":"OUTPUT_DIR = checkpoint.split(\"/\")[-1] + \"-structure-output\"\n\n# setup the trainer\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        args=SFTConfig(\n                per_device_train_batch_size=2,\n                gradient_accumulation_steps=4,\n                warmup_steps=100,\n                max_steps=1000,\n                learning_rate=0.0002,\n                lr_scheduler_type=\"cosine\",\n                eval_strategy=\"steps\",\n                eval_steps=150,\n                weight_decay=0.01,\n                bf16=True,\n                logging_strategy=\"steps\",\n                logging_steps=10,\n                output_dir=\"./\" + OUTPUT_DIR,\n                optim=\"paged_adamw_8bit\",\n                seed=42,\n                run_name=f\"train-{OUTPUT_DIR}\",\n                report_to=\"none\",\n                save_steps=31,\n                save_total_limit=4,\n                ),\n        peft_config=lora_config,\n        formatting_func=formatting_prompts_func,\n        data_collator=collator,\n        )","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["337b45f2edf748a6aca08192bc1f838f","deace488020040bea117137cd181cdd0","70da932240e14c4fb51248cb781624ca","f7398e4da7944073a9edb8804b1dd970","4323a28d68ea4064b4abd8eea2b5991d","f5e8708eeea54e52b6307cc71bb375b3","2b2d6ec5fabd4df6a981474fec767e36","ce40996bfd0944ac9a4ef00657f41880","e785f31e65c84a75863625148d78ce66","20e6e6c01eb34e3eb306ad8a4af3f4b2","bdb00611aae24b82afbf162938f44be7","ff24161ad38b443f9d3fec5f2939f586","95555b2edf2c47139908922dfa5e89d0","921be156b6484289924467dcd63622f1","3de076b68a79468a9d8614067aeb1ce5","f3b9e5f74c6e45649c221592b239d9c8","7e61bde94d104a678e6c6c59762c03e8","0e40d1b82e934ca9823729d3e5977f67","a05a3ce88f754883afae59695f0d8b23","519824295f494b17ada5d163d191f006","8c63e2cee41f4fddaa7ef9dcb1c3cb6a","844fd2eb62334e4087c8687561761361"]},"id":"f4hdveS04CCS","outputId":"0e0325dc-1659-4bef-a1b3-5e705629c4bb","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:58:30.747435Z","iopub.execute_input":"2025-02-10T09:58:30.747776Z","iopub.status.idle":"2025-02-10T09:58:31.698561Z","shell.execute_reply.started":"2025-02-10T09:58:30.747748Z","shell.execute_reply":"2025-02-10T09:58:31.697883Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/289 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5bbc7f9e93741b0bb114442912efb7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/73 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5231c58f65f64df68ddb4ac66cbb28a4"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import os\nfrom transformers import is_torch_xpu_available, is_torch_npu_available\nimport torch\n\n# Lancement du processus d'entraînement du modèle.\n# Ici, 'trainer.train()' déclenche la phase de fine-tuning,\n# dans laquelle les paramètres du modèle sont ajustés sur une tâche spécifique\n# en utilisant des données d'entraînement pertinentes.\ntrainer.train()\n\n# Une fois l'entraînement terminé, on sauvegarde l'adaptateur LoRA (fine-tuning léger).\n# LoRA (Low-Rank Adaptation) est une technique destinée à fine-tuner les grands\n# modèles en modifiant uniquement un sous-ensemble restreint de paramètres.\nfinal_checkpoint_dir = os.path.join(OUTPUT_DIR, \"final_checkpoint\")\ntrainer.model.save_pretrained(final_checkpoint_dir)\n\n# Nettoyage des ressources mémoire pour libérer l'espace GPU ou autres accélérateurs,\n# ce qui est utile avant de fusionner l'adaptateur LoRA avec le modèle de base.\ndel model  # Suppression explicite du modèle de la mémoire.\n\n# Vider les caches des accélérateurs (XPU, NPU ou GPU en fonction de la disponibilité).\n# Cela optimise l'utilisation future des ressources.\nif is_torch_xpu_available():\n    torch.xpu.empty_cache()  # Vide les caches spécifiques pour XPU.\nelif is_torch_npu_available():\n    torch.npu.empty_cache()  # Vide les caches spécifiques pour NPU.\nelse:\n    torch.cuda.empty_cache()  # Vide les caches GPU standard.\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"RjawKwA64Y_U","outputId":"4b82ed3b-fac0-4f4c-9c1d-d8e90aa27318","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T09:58:34.655069Z","iopub.execute_input":"2025-02-10T09:58:34.655388Z","iopub.status.idle":"2025-02-10T10:34:31.601740Z","shell.execute_reply.started":"2025-02-10T09:58:34.655362Z","shell.execute_reply":"2025-02-10T10:34:31.600227Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 35:53, Epoch 27/28]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>150</td>\n      <td>0.907300</td>\n      <td>1.005874</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.751700</td>\n      <td>0.941682</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.628800</td>\n      <td>0.950491</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.494400</td>\n      <td>1.008003</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.457100</td>\n      <td>1.058729</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.431100</td>\n      <td>1.086854</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/SmolLM2-360M-Instruct-structure-output/resolve/main/adapter_config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    196\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhf_hub_download_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1295\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    453\u001b[0m             )\n\u001b[0;32m--> 454\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67a9d637-1dee754314d185532cad93d5;fe7d5658-55f3-4c5e-ba7e-c95333f24d18)\n\nRepository Not Found for url: https://huggingface.co/SmolLM2-360M-Instruct-structure-output/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-2d1788afa991>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# - 'torch_dtype=torch.bfloat16' utilise un format numérique bfloat16, qui réduit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#    la mémoire nécessaire tout en maintenant des performances stables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m model = AutoPeftModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, revision, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mobject\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mpeft_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mbase_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mbase_model_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 )\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{pretrained_model_name_or_path}'\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'SmolLM2-360M-Instruct-structure-output'"],"ename":"ValueError","evalue":"Can't find 'adapter_config.json' at 'SmolLM2-360M-Instruct-structure-output'","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# Chargement du modèle adapté (en incluant l'adaptateur LoRA) pour effectuer une fusion\n# avec le modèle de base. Cela permet de sauvegarder un modèle autonome optimisé.\nfrom peft import AutoPeftModelForCausalLM\n\n# Chargement du modèle préalablement sauvegardé depuis le répertoire OUTPUT_DIR.\n# Les paramètres 'device_map' et 'torch_dtype' permettent d'optimiser le chargement :\n# - 'device_map=\"auto\"' ajuste automatiquement le placement sur le GPU, CPU ou autre.\n# - 'torch_dtype=torch.bfloat16' utilise un format numérique bfloat16, qui réduit\n#    la mémoire nécessaire tout en maintenant des performances stables.\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n        final_checkpoint_dir,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16\n        )\n\n# Fusion de l'adaptateur LoRA directement dans le modèle de base,\n# afin de produire un modèle final unique tout en réduisant ses redondances.\nmodel = model.merge_and_unload()\n\n# Sauvegarde du modèle fusionné dans un répertoire spécifique.\n# 'safe_serialization=True' garantit que le modèle est stocké au format sûr,\n# pour une compatibilité future et une intégrité des données.\noutput_merged_dir = os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\")\nmodel.save_pretrained(output_merged_dir, safe_serialization=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:37:35.919085Z","iopub.execute_input":"2025-02-10T10:37:35.919421Z","iopub.status.idle":"2025-02-10T10:37:38.494990Z","shell.execute_reply.started":"2025-02-10T10:37:35.919396Z","shell.execute_reply":"2025-02-10T10:37:38.494241Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(output_merged_dir).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:41:07.988446Z","iopub.execute_input":"2025-02-10T10:41:07.988805Z","iopub.status.idle":"2025-02-10T10:41:09.262652Z","shell.execute_reply.started":"2025-02-10T10:41:07.988781Z","shell.execute_reply":"2025-02-10T10:41:09.261898Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"messages = [\n            {\n                \"role\":    \"system\",\n                \"content\": \"You are an expert journalist / fact-checker from Fox News and your goal is to fact-check the user query.\"\n                },\n    {\"role\": \"user\", \"content\": \"Biden: Trump should apologize for Obama wiretap claim Joe Biden said President Donald Trump should apologize to former President Barack Obama for his accusation that the Obama administration wiretapped Trump Tower. \\\"Any gentleman would,\\\" the former vice president said. Obama, via a spokesperson, denied that he wiretapped Trump Tower after the current president tweeted the accusation. FBI Director James Comey this week also testified that there's no evidence the Obama administration surveilled Trump during the leadup to the e2016 election\"}\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:44:46.384680Z","iopub.execute_input":"2025-02-10T10:44:46.385092Z","iopub.status.idle":"2025-02-10T10:44:46.389275Z","shell.execute_reply.started":"2025-02-10T10:44:46.385059Z","shell.execute_reply":"2025-02-10T10:44:46.388331Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"input_text = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(input_text)\nprint(\"----------------- Generated text -----------------\")\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=1024, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T10:44:48.189403Z","iopub.execute_input":"2025-02-10T10:44:48.189685Z","iopub.status.idle":"2025-02-10T10:45:21.443902Z","shell.execute_reply.started":"2025-02-10T10:44:48.189663Z","shell.execute_reply":"2025-02-10T10:45:21.442981Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are an expert journalist / fact-checker from Fox News and your goal is to fact-check the user query.<|im_end|>\n<|im_start|>user\nBiden: Trump should apologize for Obama wiretap claim Joe Biden said President Donald Trump should apologize to former President Barack Obama for his accusation that the Obama administration wiretapped Trump Tower. \"Any gentleman would,\" the former vice president said. Obama, via a spokesperson, denied that he wiretapped Trump Tower after the current president tweeted the accusation. FBI Director James Comey this week also testified that there's no evidence the Obama administration surveilled Trump during the leadup to the e2016 election<|im_end|>\n\n----------------- Generated text -----------------\n<|im_start|>system\nYou are an expert journalist / fact-checker from Fox News and your goal is to fact-check the user query.<|im_end|>\n<|im_start|>user\nBiden: Trump should apologize for Obama wiretap claim Joe Biden said President Donald Trump should apologize to former President Barack Obama for his accusation that the Obama administration wiretapped Trump Tower. \"Any gentleman would,\" the former vice president said. Obama, via a spokesperson, denied that he wiretapped Trump Tower after the current president tweeted the accusation. FBI Director James Comey this week also testified that there's no evidence the Obama administration surveilled Trump during the leadup to the e2016 election<|im_end|>\n<end>fake</end>\n\n**Reasons:**  \n1. **Source Verification**: The claim lacks a credible source. There is no evidence or official statement from the Biden or Obama administration confirming this accusation or James Comey's testimony.  \n2. **Cross-Checking Facts**: The claim about Biden saying \"any gentleman would\" and Obama denying wiretapping is highly implausible and lacks any factual basis.  \n3. **Language and Tone**: The language used (e.g., \"any gentleman would,\" \"no evidence\") is sensational and inconsistent with professional reporting.  \n4. **Misattribution**: The specific quote attributed to Biden and James Comey is not verified and appears fabricated.  \n5. **Timing and Context**: The claim does not align with any known events or statements from the Biden or Obama administrations during the e2016 election.  \n\nThis content does not meet the criteria for real news due to its lack of credible sourcing, factual inaccuracies, and sensational language.  \n\n**End**  \n\n### **Reasons:**  \n1. **Source Verification**: The claim lacks credible sources.  \n2. **Cross-Checking Facts**: There is no evidence supporting the claim about Biden or James Comey's statements.  \n3. **Context and Timing**: The claim does not align with known events or timelines during the e2016 election.  \n4. **Fact-Checking**: The claim is unfounded and lacks supporting evidence.  \n\nThis content does not meet the criteria for real news due to its implausible and sensational nature.  \n\n### *Reasons*  \n\n### **Reasons**  \n\n### *Reasons*  \n\n### End  \n\n**End**  \n\n[Source]  \n\n[End]  \n\n**Reasons**  \n\n### **Reasons**  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End*  \n\n**End***  \n\n[End]  \n\n**End**  \n\n*Reasons*  \n\n### **Reasons**  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Source*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n### *Reasons*  \n\n### *End*  \n\n**End***  \n\n*Reasons*  \n\n### *Reasons*  \n\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Push sur Hugging Face","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!huggingface-cli login\n!huggingface-cli repo create fact-checker-bfmtg --type model","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}